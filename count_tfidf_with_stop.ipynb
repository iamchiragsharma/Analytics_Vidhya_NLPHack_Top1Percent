{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tfidf_without_stop_removal(train,create_test=True):\n",
    "    print(\"Original Train File: \", train.shape)\n",
    "    train = train.drop(train[train['user_review'].isna()].index) #Dropping na index from text\n",
    "    empty_idx = train[train['user_review'] == ' '].index\n",
    "    train = train.drop(index=empty_idx)\n",
    "    print(\"Train File After Dropping na and ' ' empty string: \",train.shape)\n",
    "    user_review = train.user_review.values\n",
    "    idx = []\n",
    "    for i,ur in enumerate(user_review): #Checking whether there are still values which are not string (nan is a float)\n",
    "        if not isinstance(ur,str):\n",
    "            idx.append(i)\n",
    "    train = train.drop(index=idx)\n",
    "    print(\"Train File After Double Check of na and ' ' empty string: \",train.shape)\n",
    "    user_review = train.user_review.values #Safe since we have updated train at each step\n",
    "    docs = nlp.pipe(user_review) #fitting it to the pipeline\n",
    "    cleaned_text = []\n",
    "    pbar = tqdm(total=len(user_review))\n",
    "    for i,doc in enumerate(docs):\n",
    "        doc_vec = []\n",
    "        for token in doc:\n",
    "            doc_vec.append(token.text)\n",
    "        cleaned_text.append(doc_vec)\n",
    "        if i%1000 == 0  and i != 0:\n",
    "            pbar.update(1000)\n",
    "    pbar.close()\n",
    "    print(\"++++++++++ CLEANING DONE +++++++++++++\")\n",
    "    print(\"+++++++++ ORIGINAL REVIEW +++++++++++\")\n",
    "    print(user_review[0])\n",
    "    print(\"+++++++++ TEXT CLEANED ++++++++++++++\")\n",
    "    print(\" \".join(cleaned_text[0]))\n",
    "    corpus = np.array([\" \".join(t) for t in cleaned_text])\n",
    "    if create_test:\n",
    "        X_train,X_test,y_train,y_test = train_test_split(corpus,train.user_suggestion.values,stratify=train.user_suggestion.values,test_size=0.2)\n",
    "        return X_train,X_test,y_train,y_test,corpus\n",
    "    else:\n",
    "        return corpus,train.user_suggestion.values,corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cleaning_without_stop_removal(test):\n",
    "    nan_idx = test[test['user_review'].isna()].index\n",
    "    test.loc[nan_idx,'user_review'] = test.loc[nan_idx,'title']\n",
    "    test_user_review = test.user_review\n",
    "    for i,ur in enumerate(test_user_review):\n",
    "        if not isinstance(ur,str):\n",
    "            print(i)\n",
    "    test_data = []\n",
    "    test_docs = nlp.pipe(test_user_review)\n",
    "    pbar = tqdm(total=len(test_user_review))\n",
    "    for i,doc in enumerate(test_docs):\n",
    "        doc_vec = []\n",
    "        for token in doc:\n",
    "            doc_vec.append(token.text)\n",
    "        test_data.append(doc_vec)\n",
    "        if i%1000 == 0  and i != 0:\n",
    "            pbar.update(1000)\n",
    "    pbar.close()\n",
    "    test_corpus = np.array([\" \".join(t) for t in test_data])\n",
    "    return test_corpus,test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/17472 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train File:  (17494, 9)\n",
      "Train File After Dropping na and ' ' empty string:  (17472, 9)\n",
      "Train File After Double Check of na and ' ' empty string:  (17472, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 1000/17472 [00:22<06:09, 44.63it/s]\u001b[A\n",
      "  6%|▌         | 1000/17472 [00:34<06:09, 44.63it/s]\u001b[A\n",
      " 11%|█▏        | 2000/17472 [00:40<05:27, 47.27it/s]\u001b[A\n",
      " 11%|█▏        | 2000/17472 [00:54<05:27, 47.27it/s]\u001b[A\n",
      " 17%|█▋        | 3000/17472 [00:57<04:48, 50.09it/s]\u001b[A\n",
      " 17%|█▋        | 3000/17472 [01:14<04:48, 50.09it/s]\u001b[A\n",
      " 23%|██▎       | 4000/17472 [01:14<04:15, 52.64it/s]\u001b[A\n",
      " 29%|██▊       | 5000/17472 [01:28<03:36, 57.67it/s]\u001b[A\n",
      " 29%|██▊       | 5000/17472 [01:44<03:36, 57.67it/s]\u001b[A\n",
      " 34%|███▍      | 6000/17472 [01:46<03:21, 56.90it/s]\u001b[A\n",
      " 40%|████      | 7000/17472 [02:02<02:59, 58.36it/s]\u001b[A\n",
      " 40%|████      | 7000/17472 [02:14<02:59, 58.36it/s]\u001b[A\n",
      " 46%|████▌     | 8000/17472 [02:16<02:34, 61.30it/s]\u001b[A\n",
      " 52%|█████▏    | 9000/17472 [02:32<02:15, 62.34it/s]\u001b[A\n",
      " 52%|█████▏    | 9000/17472 [02:44<02:15, 62.34it/s]\u001b[A\n",
      " 57%|█████▋    | 10000/17472 [02:49<02:02, 61.22it/s]\u001b[A\n",
      " 57%|█████▋    | 10000/17472 [03:04<02:02, 61.22it/s]\u001b[A\n",
      " 63%|██████▎   | 11000/17472 [03:05<01:46, 60.84it/s]\u001b[A\n",
      " 69%|██████▊   | 12000/17472 [03:23<01:31, 59.62it/s]\u001b[A\n",
      " 69%|██████▊   | 12000/17472 [03:34<01:31, 59.62it/s]\u001b[A\n",
      " 74%|███████▍  | 13000/17472 [03:39<01:13, 60.64it/s]\u001b[A\n",
      " 74%|███████▍  | 13000/17472 [03:54<01:13, 60.64it/s]\u001b[A\n",
      " 80%|████████  | 14000/17472 [03:58<01:00, 57.31it/s]\u001b[A\n",
      " 86%|████████▌ | 15000/17472 [04:13<00:41, 60.27it/s]\u001b[A\n",
      " 86%|████████▌ | 15000/17472 [04:24<00:41, 60.27it/s]\u001b[A\n",
      " 92%|█████████▏| 16000/17472 [04:25<00:22, 66.02it/s]\u001b[A\n",
      " 97%|█████████▋| 17000/17472 [04:30<00:07, 62.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++ CLEANING DONE +++++++++++++\n",
      "+++++++++ ORIGINAL REVIEW +++++++++++\n",
      "i am scared and hearing creepy voices so i will pause for a moment and write a review while i wait for my heart beat to return to atleast somewhat calmer times this game is adorable and creepy like my happy tree friends but with the graphics sceme of my childhood but more bubble and clean hello 1990s what charactes there are that isnot trying to kill me were likable and a bit odd i did do a few noob things though such as oh look a class room full of ghosts from dead children lets shine my flashlight on them and stand there staring at them or hmm creepy music i will turn around and see if i can see what is chasing me never before in a game have i been this afraid of finding a locked door\n",
      "+++++++++ TEXT CLEANED ++++++++++++++\n",
      "i am scared and hearing creepy voices so i will pause for a moment and write a review while i wait for my heart beat to return to atleast somewhat calmer times this game is adorable and creepy like my happy tree friends but with the graphics sceme of my childhood but more bubble and clean hello 1990s what charactes there are that isnot trying to kill me were likable and a bit odd i did do a few noob things though such as oh look a class room full of ghosts from dead children lets shine my flashlight on them and stand there staring at them or hmm creepy music i will turn around and see if i can see what is chasing me never before in a game have i been this afraid of finding a locked door\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"proper_cleaned_data/clean_review_tags_overview_notremoved_stopwords_frequent_nonfrequent_words.csv\")\n",
    "X_train,X_test,y_train,y_test,train_corpus = count_tfidf_without_stop_removal(train)\n",
    "# train = pd.read_csv(\"proper_cleaned_data/clean_review_tags_overview_notremoved_stopwords_frequent_nonfrequent_words.csv\")\n",
    "# X_train,y_train,train_corpus = count_tfidf_without_stop_removal(train,create_test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"proper_cleaned_data/test_clean_review_tags_overview_notremoved_stopwords_frequent_nonfrequent_words.csv\")\n",
    "X_test_sub,test_corpus = test_cleaning_without_stop_removal(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_test_count(model,corpus,X_train,y_train,X_test,y_test=\"Not passed\"):\n",
    "    cv = CountVectorizer(ngram_range=(1,2))\n",
    "    full_corpus = cv.fit_transform(corpus)\n",
    "    X_train_cv = cv.transform(X_train)\n",
    "    X_test_cv = cv.transform(X_test)\n",
    "    model.fit(X_train_cv,y_train)\n",
    "    y_pred = model.predict(X_test_cv)\n",
    "    if y_test != \"Not passed\":\n",
    "        print(f1_score(y_test,y_pred))\n",
    "    else:\n",
    "        sub = pd.DataFrame()\n",
    "        sub['review_id'] = test.review_id\n",
    "        sub['user_suggestion'] = y_pred\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_test_count_tfidf(model,corpus,X_train,y_train,X_test,y_test=\"Not passed\"):\n",
    "    cv = CountVectorizer(ngram_range=(1,2))\n",
    "    full_corpus = cv.fit_transform(corpus)\n",
    "    tf = TfidfTransformer()\n",
    "    tf.fit(full_corpus)\n",
    "    X_train_cv = cv.transform(X_train)\n",
    "    X_test_cv = cv.transform(X_test)\n",
    "    X_train_cv = tf.transform(X_train_cv)\n",
    "    X_test_cv = tf.transform(X_test_cv)\n",
    "    model.fit(X_train_cv,y_train)\n",
    "    y_pred = model.predict(X_test_cv)\n",
    "    if y_test != \"Not passed\":\n",
    "        print(f1_score(y_test,y_pred))\n",
    "    else:\n",
    "        sub = pd.DataFrame()\n",
    "        sub['review_id'] = test.review_id\n",
    "        sub['user_suggestion'] = y_pred\n",
    "        return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_final = SVC(kernel='linear',C=0.01)\n",
    "corpus = np.concatenate((corpus,test_corpus))\n",
    "sub = local_test_count(svm_final,corpus,X_train,y_train,X_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7258476121035362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chirag/Career/Codes/JanataHackNLP/venv/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "svm_final = SVC(kernel='linear', C=0.01)\n",
    "local_test_count_tfidf(svm_final,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"svm_linear_0.01.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear')\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear',C=0.1)\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear',C=10)\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear',C=100)\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear',C=0.01)\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel='linear',C=0.001)\n",
    "local_test(svm_linear,train_corpus,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,2))\n",
    "# full_corpus = cv.fit_transform(train_corpus)\n",
    "# X_train_cv = cv.transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test)\n",
    "# lr = LogisticRegression(max_iter=800)\n",
    "# lr.fit(X_train_cv,y_train)\n",
    "# y_pred = lr.predict(X_test_cv)\n",
    "# f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,2))\n",
    "# full_corpus = cv.fit_transform(train_corpus)\n",
    "# X_train_cv = cv.transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test)\n",
    "# lr = MultinomialNB()\n",
    "# lr.fit(X_train_cv,y_train)\n",
    "# y_pred = lr.predict(X_test_cv)\n",
    "# f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,2))\n",
    "# #corpus = np.concatenate((corpus,test_corpus))\n",
    "# full_corpus = cv.fit_transform(train_corpus)\n",
    "# X_train_cv = cv.transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = TfidfTransformer()\n",
    "# tf.fit(full_corpus)\n",
    "# X_train_cv = tf.transform(X_train_cv)\n",
    "# X_test_cv = tf.transform(X_test_cv)\n",
    "# lr = LogisticRegression(max_iter=800)\n",
    "# lr.fit(X_train_cv,y_train)\n",
    "# y_pred = lr.predict(X_test_cv)\n",
    "# f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,2))\n",
    "# corpus = np.concatenate((corpus,test_corpus))\n",
    "# full_corpus = cv.fit_transform(corpus)\n",
    "# X_train_cv = cv.transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test_sub)\n",
    "# lr = LogisticRegression(max_iter=800)\n",
    "# lr.fit(X_train_cv,y_train)\n",
    "# y_pred = lr.predict(X_test_cv)\n",
    "# #Highest submission 85 with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = CountVectorizer(ngram_range=(1,2))\n",
    "# #corpus = np.concatenate((corpus,test_corpus))\n",
    "# full_corpus = cv.fit_transform(train_corpus)\n",
    "# X_train_cv = cv.transform(X_train)\n",
    "# X_test_cv = cv.transform(X_test)\n",
    "# xgb = XGBClassifier()\n",
    "# xgb.fit(X_train_cv,y_train)\n",
    "# y_pred = xgb.predict(X_test_cv)\n",
    "# f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['review_id'] = test.review_id\n",
    "sub['user_suggestion'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission_tfidf_stop.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
